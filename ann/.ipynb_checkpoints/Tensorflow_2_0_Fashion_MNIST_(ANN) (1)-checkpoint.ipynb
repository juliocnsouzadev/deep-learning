{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJr4nMgyb9oS"
   },
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://storage.googleapis.com/kaggle-datasets-images/2243/3791/9384af51de8baa77f6320901f53bd26b/dataset-cover.png\" />\n",
    "  Image source: https://www.kaggle.com/\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUJOgKC34gKp"
   },
   "source": [
    "# Fashion MNIST\n",
    "## An MNIST-like dataset of 70,000 28x28 labeled fashion images\n",
    "### Context\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
    "\n",
    "Zalando seeks to replace the original MNIST dataset\n",
    "\n",
    "### Content\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n",
    "\n",
    "Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0 T-shirt/top\n",
    "1 Trouser\n",
    "2 Pullover\n",
    "3 Dress\n",
    "4 Coat\n",
    "5 Sandal\n",
    "6 Shirt\n",
    "7 Sneaker\n",
    "8 Bag\n",
    "9 Ankle boot \n",
    "\n",
    "TL;DR\n",
    "\n",
    "Each row is a separate image\n",
    "Column 1 is the class label.\n",
    "Remaining columns are pixel numbers (784 total).\n",
    "Each value is the darkness of the pixel (1 to 255)\n",
    "\n",
    "https://www.kaggle.com/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVF9I9v6Zipb"
   },
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WS6fShx3Za4O"
   },
   "outputs": [],
   "source": [
    "#no need do install it on colab\n",
    "#!pip install tensorflow-gpu==2.0.0.alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjlnjPnjWYFw"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yt0-hrch6rZw",
    "outputId": "61cf8409-2053-40d5-ee73-b5a320a1abf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "print(\"Tensorflow Version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2OiUS-kWkJU"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdfoFiEEXYj1"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "-lCgz6UC8pKT",
    "outputId": "355a5625-7249-43c5-e92e-fe87668c1407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYxeEHzDXdSs"
   },
   "source": [
    "### Normalizing the images\n",
    "\n",
    "We divide each pixel of the image in the training and test sets by the maximum number of pixels (255).\n",
    "\n",
    "In this way each pixel will be in the range [0, 1]. By normalizing images we make sure that our model (ANN) trains faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvWzsB3G9IU8"
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBacLmGIX0Es"
   },
   "source": [
    "### Reshaping the dataset\n",
    "\n",
    "Since we are building a fully connected network, we reshape the training set and the test set to be into the vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKmtunXL5lOs"
   },
   "outputs": [],
   "source": [
    "# Since each image's dimension is 28x28, we reshape the full dataset to [-1 (all elements), height *\n",
    "def reshape(data):\n",
    "  return data.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Tao7pom-grn"
   },
   "outputs": [],
   "source": [
    "X_train = reshape(X_train)\n",
    "X_test = reshape(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5aDsaYSYmXD"
   },
   "source": [
    "## Building an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l30aZ6-GYtUP"
   },
   "source": [
    "### Defining the model\n",
    "\n",
    "Simply define an object of the Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmfogzmn9kqv"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNzLOAK5Y-mR"
   },
   "source": [
    "### Adding fully-connected hidden layer\n",
    "### Adding a second layer with Dropout\n",
    "\n",
    "Dropout is a Regularization technique where we randomly set neurons in a layer to zero. That way while training those neurons won't be updated. Because some percentage of neurons won't be updated the whole training process is long and we have less chance for overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBsfDyGE-FX5"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784, )))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGqvyDvNbzwN"
   },
   "source": [
    "### Adding the output layer\n",
    "\n",
    "- units: number of classes (10 in the Fashion MNIST dataset)\n",
    "- activation: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmkUuF9Y-3mG"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rRsMjsvcOua"
   },
   "source": [
    "### Compiling the model\n",
    "\n",
    "- Optimizer: Adam\n",
    "- Loss: Sparse softmax (categorical) crossentropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbW3xeRK_CrN"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['sparse_categorical_accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "8dQOL_EtChrN",
    "outputId": "2bf25146-50d7-4c3b-f302-7f38f2ca1feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 270,218\n",
      "Trainable params: 270,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kxIIFU1cany"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s-_oLiE0_3A2",
    "outputId": "6610953c-7cda-4f88-962c-58bc71c699b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1331 - sparse_categorical_accuracy: 0.9505\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1333 - sparse_categorical_accuracy: 0.9502\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1347 - sparse_categorical_accuracy: 0.9506\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1313 - sparse_categorical_accuracy: 0.9520\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1356 - sparse_categorical_accuracy: 0.9505\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1366 - sparse_categorical_accuracy: 0.9498\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1391 - sparse_categorical_accuracy: 0.9492\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1300 - sparse_categorical_accuracy: 0.9505\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1317 - sparse_categorical_accuracy: 0.9508\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1330 - sparse_categorical_accuracy: 0.9516\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1331 - sparse_categorical_accuracy: 0.9508\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1290 - sparse_categorical_accuracy: 0.9520\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1343 - sparse_categorical_accuracy: 0.9506\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1340 - sparse_categorical_accuracy: 0.9507\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.9511\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1278 - sparse_categorical_accuracy: 0.9530\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1329 - sparse_categorical_accuracy: 0.9503\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1309 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9519\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1355 - sparse_categorical_accuracy: 0.9515\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1288 - sparse_categorical_accuracy: 0.9519\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1291 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1340 - sparse_categorical_accuracy: 0.9511\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1281 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1279 - sparse_categorical_accuracy: 0.9535\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1277 - sparse_categorical_accuracy: 0.9525\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1318 - sparse_categorical_accuracy: 0.9515\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1330 - sparse_categorical_accuracy: 0.9510\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9536\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9534\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1266 - sparse_categorical_accuracy: 0.9541\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1272 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1289 - sparse_categorical_accuracy: 0.9535\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9532\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9529\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1254 - sparse_categorical_accuracy: 0.9539\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1243 - sparse_categorical_accuracy: 0.9551\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1283 - sparse_categorical_accuracy: 0.9538\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1279 - sparse_categorical_accuracy: 0.9532\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9530\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9538\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1250 - sparse_categorical_accuracy: 0.9536\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1231 - sparse_categorical_accuracy: 0.9544\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1233 - sparse_categorical_accuracy: 0.9541\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1216 - sparse_categorical_accuracy: 0.9551\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1258 - sparse_categorical_accuracy: 0.9542\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9557\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1214 - sparse_categorical_accuracy: 0.9557\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.1246 - sparse_categorical_accuracy: 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd44a8f7748>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mj23nxmtcrhd"
   },
   "source": [
    "### Model evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-nQCioOmAL7i",
    "outputId": "b38a4dc2-66c3-4da3-f302-9d92f627df5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4293 - sparse_categorical_accuracy: 0.8968\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ozv2YVlxcx1h",
    "outputId": "d2de864b-c3b3-4428-d254-bc32d7e6c613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8967999815940857\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tensorflow 2.0 - Fashion MNIST (ANN)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
